\documentclass[12pt]{article}

% Essential packages for encoding, math, and formatting
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm, mathtools, bm} % Enhanced math support
\usepackage{geometry}
\geometry{a4paper, margin=0.8in}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}
\usepackage{microtype} % Improved typography
\usepackage{tocloft} % Table of contents customization
\usepackage{abstract} % For abstract environment
\usepackage{mathptmx} % Times-compatible math font
\usepackage{xcolor} % For colored text
\usepackage{tikz} % For mathematical diagrams
\usepackage{enumitem} % Better list formatting

% Custom math commands for enhanced notation
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\transpose}[1]{#1^{\mathsf{T}}}
\newcommand{\norm}[1]{\left\|\,#1\,\right\|}
\newcommand{\abs}[1]{\left|\,#1\,\right|}
\newcommand{\inner}[2]{\langle\,#1,\,#2\,\rangle}

% Define theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}{Lemma}[subsection]
\newtheorem{proposition}{Proposition}[subsection]
\newtheorem{remark}{Remark}[subsection]

\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{1cm}
    {\huge \textbf{From Projection to Perception: \\ A Mathematical Exploration of \\ Shadow-based Neural Reconstruction}\par}
    \vspace{1.5cm}
    {\normalsize
    A research report submitted to the Scientific Committee of the Hang Lung Mathematics Award\par}
    \vspace{1cm}
    {\normalsize \textbf{Team Number}\par 2596873\par}
    \vspace{0.5cm}
    {\normalsize \textbf{Team Members}\par Wong Yuk To, Hung Kwong Lam \\ Cheung Tsz Lung, Chan Ngo Tin, Zhou Lam Ho\par}
    \vspace{0.5cm}
    {\normalsize \textbf{Teacher}\par Mr. Chan Ping Ho\par}
    \vspace{0.5cm}
    {\normalsize \textbf{School}\par Po Leung Kuk Celine Ho Yam Tong College\par}
    \vspace{0.5cm}
    {\normalsize \textbf{Date}\par \today\par}
    \vspace{2cm}

% Abstract
\begin{abstract}
\raggedright
This paper explores \textsc{ShadowNeuS} \hyperlink{[LWX23]}{[LWX23]}, a neural network that reconstructs 3D geometry from single-view camera images using shadow and light cues. Unlike traditional 3D reconstruction methods relying on multi-view cameras or sensors, \textsc{ShadowNeuS} leverages a neural signed distance field (SDF) for accurate 3D geometry reconstruction. We analyze the training process and uncover its connections to projective geometry, spatial reasoning in $\R^3$, and the neural network's learned geometric representation of space.
\end{abstract}

\end{titlepage}

% Table of contents
\tableofcontents

\newpage
\section{Background} \label{sec:background}

\subsection{What is 3D Reconstruction from Images?} \label{sec:reconstruction_intro}

The goal of 3D reconstruction is to recover the structure of a 3D scene using only 2D images.

\begin{definition}[3D Scene Representation] \label{def:scene_repr} ~\\
A 3D scene is represented by a set of points $\vect{P} = [P_x, P_y, P_z]^\mathsf{T} \in \R^3$ in Euclidean space.
\end{definition}

\begin{definition}[Image Projection] \label{def:image_proj} ~\\
Each image $I_n$ of the 3D scene records a set of pixel coordinates $\vect{p} = [p_x, p_y]^\mathsf{T} \in \R^2$.
\end{definition}

The process of capturing a 3D point in a 2D image $I_n$ can be modeled as a projection function $\pi_n$:

\vspace{0.5em}
\begin{equation}
\boxed{\pi_n: \R^3 \to \R^2, \quad [P_x, P_y, P_z]^\mathsf{T} \mapsto [p_x, p_y]^\mathsf{T}} \label{eq:proj}
\end{equation}
\vspace{0.5em}

This projection function represents how a camera maps a 3D point to a 2D pixel in the $n$-th image.

To reconstruct the 3D scene, we need to solve the \textbf{inverse problem} $\pi_n^{-1}$:

\vspace{0.5em}
\begin{equation}
\boxed{\pi_n^{-1}(\vect{p}) = \left\{ \vect{P} \in \R^3 \mid \pi_n(\vect{P}) = \vect{p} \right\}} \label{eq:invproj}
\end{equation}
\vspace{0.5em}

However, this inverse problem is typically \textbf{ill-posed}, as multiple 3D points may project to the same 2D pixel, leading to ambiguity. We will detail this in Section \ref{sec:inverse_problem}.

\subsection{Information Encoded in 2D Images} \label{sec:image_info}

A 2D image $I_n$ can provide multiple types of information encoded as mathematical structures:

\subsubsection*{Information Available from an Image:}
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Pixel coordinates}: $\vect{p} = [p_x, p_y]^\mathsf{T} \in \R^2$, represents the spatial location of each pixel
    
    \item \textbf{Color values}: $C_n(\vect{p}) = [r, g, b]^\mathsf{T} \in [0,1]^3$, represents the RGB tristimulus values
    
    \item \textbf{RGB gradient matrix}:
    \begin{equation}
        \nabla C_n(\vect{p}) = \begin{bmatrix}
            \frac{\partial r}{\partial p_x} & \frac{\partial r}{\partial p_y} \\[0.5em]
            \frac{\partial g}{\partial p_x} & \frac{\partial g}{\partial p_y} \\[0.5em]
            \frac{\partial b}{\partial p_x} & \frac{\partial b}{\partial p_y}
        \end{bmatrix} \in \R^{3 \times 2} \label{eq:gradient}
    \end{equation}
    This Jacobian matrix captures local intensity variations, indicating edges or texture information.
    
    \item \textbf{Learned feature embedding}: $\phi(I_n)(\vect{p}) \in \R^d$, represents high-dimensional features extracted via neural networks like CNNs
\end{enumerate}

These data structures result from projecting 3D geometry through camera optics, where $C_n(\vect{p})$ corresponds to visible surface reflectance and $\nabla C_n(\vect{p})$ encodes geometric boundaries.

\newpage

\subsection{The Forward Projection: From 3D World to 2D Image} \label{sec:forward_projection}

We formalize the perspective projection process using homogeneous coordinates and transformation matrices.

\subsubsection*{Camera Parameter Matrices:}

\begin{definition}[Extrinsic Parameters] \label{def:extrinsic} ~\\
The world-to-camera transformation is characterized by:
\begin{align}
\text{Camera center: } &\quad \vect{C} = [C_x, C_y, C_z]^\mathsf{T} \in \R^3 \\
\text{Rotation matrix: } &\quad R = \begin{bmatrix}
    r_{11} & r_{12} & r_{13} \\
    r_{21} & r_{22} & r_{23} \\
    r_{31} & r_{32} & r_{33}
\end{bmatrix} \in \text{SO}(3) \\
\text{Translation vector: } &\quad \vect{t} = -R \vect{C} \in \R^3
\end{align}
\end{definition}

\begin{definition}[Intrinsic Parameters] \label{def:intrinsic} ~\\
The camera's internal geometry is encoded by:
\begin{equation}
K = \begin{bmatrix}
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1
\end{bmatrix} \in \R^{3 \times 3} \label{eq:intrinsic}
\end{equation}
where $(f_x, f_y)$ are focal lengths in pixels and $(c_x, c_y)$ is the principal point.
\end{definition}

\subsubsection*{Forward Projection Pipeline:}

\begin{proposition}[Perspective Projection Transform] \label{prop:projection_pipeline} ~\\
The complete forward projection involves three sequential transformations:
\begin{enumerate}[label=\textbf{Step \arabic*:}]
    \item \textbf{World to camera coordinates}
    \begin{equation}
        \vect{P}_{\text{cam}} = R \vect{P} + \vect{t}
    \end{equation}
    
    \item \textbf{Camera to image coordinates}
    \begin{equation}
        \vect{P}_{\text{hom}} = K \vect{P}_{\text{cam}} = \begin{bmatrix} p_x' \\ p_y' \\ z' \end{bmatrix}
    \end{equation}
    
    \item \textbf{Perspective division}
    \begin{equation}
        \vect{p} = \begin{bmatrix} p_x \\ p_y \end{bmatrix} = \frac{1}{z'} \begin{bmatrix} p_x' \\ p_y' \end{bmatrix}, \quad z' \neq 0
    \end{equation}
\end{enumerate}
\end{proposition}

The complete transformation matrix can be expressed as:

\vspace{0.5em}
\begin{equation}
\boxed{\vect{P}_{\text{hom}} = K [R \mid \vect{t}] \begin{bmatrix} \vect{P} \\ 1 \end{bmatrix} , \quad \vect{p} = \frac{1}{z'} \begin{bmatrix} p_x' \\ p_y' \end{bmatrix}} \label{eq:forward}
\end{equation}
\vspace{0.5em}

\newpage

\subsection{The Inverse Problem: From 2D Image to 3D World} \label{sec:inverse_problem}

We now tackle the fundamental challenge of inverting the projection function.

\begin{lemma}[Camera Ray Parametrization] \label{lem:camera_ray} ~\\
Given a pixel $\vect{p} = [p_x, p_y]^\mathsf{T}$ and camera parameters $(K, R, \vect{C})$, the corresponding 3D points form a ray:

\vspace{0.25em}
\begin{equation}
\boxed{\vect{P}(\lambda) = \vect{C} + \lambda \cdot \vect{d}, \quad \lambda > 0} \label{eq:camera_ray}
\end{equation}
\vspace{0.25em}

where the ray direction is:

\vspace{0.25em}
\begin{equation}
\boxed{\vect{d} = R^{-1} K^{-1} \begin{bmatrix} p_x \\ p_y \\ 1 \end{bmatrix}} \label{eq:ray_direction}
\end{equation}
\vspace{0.25em}
\begin{remark}[Normalization] \label{rmk:normalization} ~\\
The direction vector $\vect{d}$ can optionally be normalized to unit length for physical ray tracing but not strictly necessary for the ray parametrization.
\end{remark}
\end{lemma}

\begin{proof}
Starting from the forward projection equation \eqref{eq:forward}:
\begin{align}
K (R \vect{P} + \vect{t}) &= z' \begin{bmatrix} p_x \\ p_y \\ 1 \end{bmatrix} \\
R \vect{P} + \vect{t} &= z' K^{-1} \begin{bmatrix} p_x \\ p_y \\ 1 \end{bmatrix} \\
\vect{P} &= R^{-1} \left( z' K^{-1} \begin{bmatrix} p_x \\ p_y \\ 1 \end{bmatrix} - \vect{t} \right)
\end{align}

Since $\vect{t} = -R \vect{C}$, we have $-R^{-1} \vect{t} = \vect{C}$. Setting $\lambda = z'$:
\begin{equation}
\vect{P}(\lambda) = \vect{C} + \lambda \cdot R^{-1} K^{-1} \begin{bmatrix} p_x \\ p_y \\ 1 \end{bmatrix}
\end{equation}
\end{proof}

\begin{proposition}[Ill-posed Nature of Single-View Reconstruction] \label{prop:illposed} ~\\
The inverse projection problem is fundamentally \textbf{ill-posed} because:
\begin{enumerate}[label=(\alph*)]
    \item The depth parameter $\lambda$ is undetermined
    \item Each pixel $\vect{p}$ defines a ray of infinitely many possible 3D points
    \item Additional constraints are required for unique reconstruction
\end{enumerate}
\end{proposition}

\newpage

\subsection{Cues for Solving the Inverse Problem} \label{sec:cues}

To achieve unique reconstruction, we require additional information such as:
\begin{itemize}
    \item \textbf{Stereo correspondence}: Multiple viewpoints providing triangulation
    \item \textbf{Depth sensors}: Direct measurement of $\lambda$
    \item \textbf{Shadow constraints}: Geometric relationships via light ray intersections
\end{itemize}

\section{Shadows as a Geometric Constraint} \label{sec:shadows}

We formalize how shadows encode geometric information to constrain 3D reconstruction. By analyzing light transport and occlusion, we derive conditions enabling depth recovery from single-view images, leveraging shadows as powerful geometric cues.

\subsection{Light Ray and Shadow Geometry} \label{sec:light_ray_geometry}

\begin{definition}[Light Ray] \label{def:light_ray} ~\\
Given a point light source $\vect{L} \in \R^3$ and a surface point $\vect{P} \in \R^3$, the light ray is:
\begin{equation}
\boxed{r(t) = \vect{L} + t(\vect{P} - \vect{L}), \quad t \in [0,1]} \label{eq:light_ray}
\end{equation}
\end{definition}

\begin{definition}[Shadow Occlusion Test] \label{def:shadow_occlusion} ~\\
A point $\vect{P}$ is in shadow if there exists $t \in (0,1)$ such that the light ray intersects a surface $\mathcal{S}$:
\begin{equation}
\boxed{r(t) \cap \mathcal{S} \neq \emptyset, \quad t \in (0,1)}
\end{equation}
\end{definition}

\begin{remark}[Physical Interpretation] \label{rmk:occlusion_physics} ~\\
The interval $(0,1)$ excludes the light source ($t=0$) and the target point ($t=1$), ensuring the test checks for obstructions between $\vect{L}$ and $\vect{P}$. This models physical light transport where occlusion by another surface causes a shadow.
\end{remark}

\subsection{Shadow Boundary and Surface Partitioning} \label{sec:shadow_boundary}

\begin{theorem}[Tangency Condition] \label{thm:tangency} ~\\
A point $\vect{Q} \in \mathcal{S}$ lies on the shadow boundary if and only if the light direction is tangent to the surface:
\begin{equation}
\boxed{(\vect{Q} - \vect{L}) \cdot \vect{n}(\vect{Q}) = 0} \label{eq:tangency}
\end{equation}
where $\vect{n}(\vect{Q})$ is the outward unit normal at $\vect{Q}$.
\end{theorem}

\begin{proof}
At the shadow boundary, the light ray $r(t)$ grazes the surface $\mathcal{S}$ at $\vect{Q}$, meaning the direction $\vect{Q} - \vect{L}$ lies in the tangent plane. Thus, it is perpendicular to the surface normal $\vect{n}(\vect{Q})$, yielding $(\vect{Q} - \vect{L}) \cdot \vect{n}(\vect{Q}) = 0$.
\end{proof}

\begin{definition}[Shadow Boundary Set] \label{def:boundary_set} ~\\
The 3D shadow boundary is:
\begin{equation}
\boxed{\mathcal{B} = \{\vect{Q} \in \mathcal{S} \mid (\vect{Q} - \vect{L}) \cdot \vect{n}(\vect{Q}) = 0\}} \label{eq:boundary_set}
\end{equation}
\end{definition}

\newpage

\begin{proposition}[Surface Illumination Partition] \label{prop:illumination_partition} ~\\
The surface $\mathcal{S}$ is partitioned into three disjoint regions based on the light direction:
\begin{align}
\mathcal{S}_{\text{lit}} &= \{\vect{P} \in \mathcal{S} \mid (\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) > 0\} && \text{(illuminated)} \label{eq:lit_region} \\
\mathcal{S}_{\text{shadow}} &= \{\vect{P} \in \mathcal{S} \mid (\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) < 0\} && \text{(attached shadow)} \label{eq:shadow_region} \\
\mathcal{B} &= \{\vect{P} \in \mathcal{S} \mid (\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) = 0\} && \text{(shadow boundary)} \label{eq:boundary_region}
\end{align}
\end{proposition}

\begin{remark}[Geometric Interpretation] \label{rmk:illum_sign} ~\\
The sign of $(\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P})$ reflects the angle between the light direction and the surface normal, determining whether a point is lit, shadowed, or on the boundary.
\end{remark}

\subsection{Cast Shadow Regions} \label{sec:cast_shadow}

\begin{definition}[Cast Shadow Region] \label{def:cast_shadow} ~\\
For an occluding surface $\mathcal{S}_1$ and a receiving surface $\mathcal{S}_2$, the cast shadow region is:
\begin{equation}
\boxed{\mathcal{C}_{1 \to 2} = \{\vect{P} \in \mathcal{S}_2 \mid \exists t \in (0,1): \vect{L} + t(\vect{P} - \vect{L}) \in \mathcal{S}_1\}} \label{eq:cast_shadow}
\end{equation}
\end{definition}

\begin{remark}[Cast Shadow Formation] \label{rmk:cast_shadow_formation} ~\\
A point $\vect{P} \in \mathcal{S}_2$ is in the cast shadow if the light ray from $\vect{L}$ to $\vect{P}$ is blocked by $\mathcal{S}_1$. For multiple occluders $\mathcal{S}_1, \mathcal{S}_2, \ldots, \mathcal{S}_k$, a point is shadowed if it lies in $\bigcup_{i=1}^k \mathcal{C}_{i \to \text{target}}$. Self-shadowing occurs when $\mathcal{S}_1 = \mathcal{S}_2$, as a surface may occlude itself.
\end{remark}

\begin{definition}[Cast Shadow Boundary] \label{def:cast_boundary} ~\\
The cast shadow boundary on $\mathcal{S}_2$ is:
\begin{equation}
\boxed{\partial\mathcal{C}_{1 \to 2} = \{\vect{P} \in \mathcal{S}_2 \mid \vect{P} = \vect{Q} + s(\vect{Q} - \vect{L}), \vect{Q} \in \mathcal{B}_1, s > 0\}} \label{eq:cast_boundary}
\end{equation}
where $\mathcal{B}_1$ is the shadow boundary on $\mathcal{S}_1$ (Equation \eqref{eq:boundary_set}).
\end{definition}

\begin{proposition}[Multi-Surface Cast Shadows and Boundaries] \label{prop:multi_cast} ~\\
For $k$ occluders, the total cast shadow region and its boundary on a target surface are:
\begin{equation}
\boxed{\mathcal{C}_{\text{total}} = \bigcup_{i=1}^k \mathcal{C}_{i \to \text{target}} \label{eq:total_cast_region}}
\end{equation}
\begin{equation}
\boxed{\partial\mathcal{C}_{\text{total}} = \bigcup_{i=1}^k \partial\mathcal{C}_{i \to \text{target}} \label{eq:total_cast_boundary}}
\end{equation}
where $\mathcal{C}_{i \to \text{target}}$ is the cast shadow region from $\mathcal{S}_i$ and $\partial\mathcal{C}_{i \to \text{target}}$ is its boundary.
\end{proposition}

\subsection{Shadow Regions in Images} \label{sec:shadow_in_image}

\begin{definition}[Image Shadow and Lit Regions] \label{def:image_shadow_regions} ~\\
Given the projection function $\pi: \R^3 \to \R^2$ (Equation \eqref{eq:proj}), the image $I$ partitions into:
\begin{align}
\Omega_{\text{lit}}^{\text{obs}} &= \{\pi(\vect{P}) \mid \vect{P} \in \mathcal{S}_{\text{lit}}\} && \text{(lit region)} \label{eq:image_lit} \\
\Omega_{\text{shadow}}^{\text{obs}} &= \{\pi(\vect{P}) \mid \vect{P} \in \mathcal{S}_{\text{shadow}} \cup \mathcal{C}_{\text{total}}\} && \text{(shadow region)} \label{eq:image_shadow} \\
\partial\Omega_{\text{shadow}}^{\text{obs}} &= \{\pi(\vect{P}) \mid \vect{P} \in \mathcal{B} \cup \partial\mathcal{C}_{\text{total}}\} && \text{(shadow boundary)} \label{eq:image_boundary}
\end{align}
where $\mathcal{S}_{\text{lit}}$, $\mathcal{S}_{\text{shadow}}$, $\mathcal{B}$, and $\mathcal{C}_{\text{total}}$ are defined in Equations \eqref{eq:lit_region}, \eqref{eq:shadow_region}, \eqref{eq:boundary_region}, and \eqref{eq:total_cast_region}.
\end{definition}

\newpage

\begin{remark}[Relation between shadow region and image region] \label{rmk:shadow_projection} ~\\
The forward projection $\pi$ maps 3D surface points to 2D image pixels. For example, a 3D point $\vect{P}$ on a lit surface projects to pixel $\vect{p} = \pi(\vect{P})$ in the bright image region $\Omega_{\text{lit}}^{\text{obs}}$. Similarly, shadowed 3D points project to dark pixels in $\Omega_{\text{shadow}}^{\text{obs}}$. This creates a direct correspondence between 3D geometry and what we observe in the image.
\end{remark}

\begin{definition}[Shadow Consistency Loss] \label{def:shadow_loss} ~\\
The shadow consistency loss measures how well our reconstructed 3D geometry matches the observed shadows:
\begin{equation}
\mathcal{L}_{\text{shadow}} = w_1 \cdot \mathcal{L}_{\text{lit-mismatch}} + w_2 \cdot \mathcal{L}_{\text{shadow-mismatch}} + w_3 \cdot \mathcal{L}_{\text{boundary-mismatch}}
\end{equation}
where $\mathcal{L}_{\text{lit-mismatch}}$, $\mathcal{L}_{\text{shadow-mismatch}}$, and $\mathcal{L}_{\text{boundary-mismatch}}$ penalize pixels that are misclassified as lit, shadow, or boundary regions respectively, and $w_1, w_2, w_3$ are weighting coefficients.
\end{definition}

\begin{remark}[Purpose of Shadow Consistency Loss] \label{rmk:shadow_loss_purpose} ~\\
This loss acts as a geometric constraint: if we observe a shadow at pixel $\vect{p}$, then the 3D point $\pi^{-1}(\vect{p})$ must be occluded by some surface. The loss penalizes inconsistencies, forcing the reconstruction to respect shadow boundaries and create geometrically reasonable surfaces.
\end{remark}

\subsection{Shadows as Cues for 3D Reconstruction} \label{sec:shadow_cues}

\subsubsection*{Geometric Information Encoded in Shadows} \label{sec:shadow_geom_info}

Shadows provide critical geometric constraints for single-view 3D reconstruction, leveraging light-surface interactions.

\begin{proposition}[Shadow-Derived Geometric Cues] \label{prop:shadow_cues} ~\\
From observed shadows, we can infer:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Surface Orientation:} Points in attached shadows satisfy
    \[
    (\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) < 0
    \]
    as in Equation~\eqref{eq:shadow_region}, indicating that the surface at $\vect{P}$ is facing away from the light source.

    \item \textbf{Tangency Constraints at Boundaries:} For points on the shadow boundary, the incoming light direction lies in the surface's tangent plane:
    \[
    (\vect{Q} - \vect{L}) \cdot \vect{n}(\vect{Q}) = 0
    \]
    (Equation~\eqref{eq:tangency}). This geometric constraint localizes the transition between lit and shadowed regions.

    \item \textbf{Relative Depth from Cast Shadows:} If $\vect{P} \in \mathcal{C}_{1 \to 2}$ (cast shadow region), then the occluding surface $\mathcal{S}_1$ must be geometrically in front of $\mathcal{S}_2$ along the light direction, as defined in Equation~\eqref{eq:cast_shadow}. This gives ordinal depth information.

    \item \textbf{Occlusion Structure:} The presence of a shadow implies the existence of an occluder blocking the light ray from $\vect{L}$ to $\vect{P}$, indicating some intermediate geometry between the light and the shadowed surface point.
\end{enumerate}
\end{proposition}

\begin{remark}[Depth Ordering via Cast Shadows] \label{rmk:depth_order} ~\\
Cast shadows provide explicit ordering information. If a point $\vect{P}$ on surface $\mathcal{S}_2$ is shadowed due to surface $\mathcal{S}_1$, then $\mathcal{S}_1$ must lie between the light source $\vect{L}$ and $\vect{P}$ along the ray. This insight can be directly encoded into optimization or inference schemes.
\end{remark}

\newpage

\subsubsection*{From Shadow Observation to Depth Estimation} \label{sec:shadow_depth}

When a pixel $\vect{p} \in \partial\Omega_{\text{shadow}}^{\text{obs}}$ is observed to lie on the shadow boundary in the image, the corresponding 3D point $\vect{P}$ must satisfy the tangency condition in 3D space. Since $\vect{P}$ lies on the camera ray:

\[
\vect{P}(\lambda) = \vect{C} + \lambda \vect{d}
\]

we can substitute this into the tangency condition to derive a nonlinear constraint on depth $\lambda$:

\begin{theorem}[Single-View Depth Recovery from Shadow Boundaries] \label{thm:depth_from_shadow} ~\\
Given a pixel $\vect{p} \in \partial\Omega_{\text{shadow}}^{\text{obs}}$, the corresponding 3D point $\vect{P} = \vect{C} + \lambda \vect{d}$ lies on the shadow boundary $\mathcal{B}$ if and only if:
\begin{equation}
\boxed{(\vect{C} + \lambda \vect{d} - \vect{L}) \cdot \vect{n}(\vect{C} + \lambda \vect{d}) = 0} \label{eq:shadow_depth_constraint}
\end{equation}
This constraint defines a scalar equation in the unknown $\lambda$, assuming the surface normal is known or estimated.
\end{theorem}

\begin{proof}
Since $\vect{p} \in \partial\Omega_{\text{shadow}}^{\text{obs}}$, it must correspond to a surface point $\vect{P}$ in $\mathcal{B}$ or $\partial\mathcal{C}_{\text{total}}$ (Equation~\eqref{eq:image_boundary}). If $\vect{P} \in \mathcal{B}$, then by the tangency condition (Equation~\eqref{eq:tangency}), we substitute $\vect{P} = \vect{C} + \lambda \vect{d}$ into:
\[
(\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) = 0
\]
to yield the boxed equation.
\end{proof}

\subsubsection*{Limitations and Computational Challenges} \label{sec:shadow_limitations}

While shadows provide rich constraints, several technical difficulties must be addressed for practical reconstruction:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Dependence on Surface Normals:} Equation~\eqref{eq:shadow_depth_constraint} depends on $\vect{n}(\vect{P})$, which in turn depends on the unknown surface geometry. This creates a circular dependency in the reconstruction process.
    \item \textbf{Non-Differentiable Occlusion Tests:} The condition $r(t) \cap \mathcal{S} \neq \emptyset$ is inherently non-differentiable, which limits the applicability of gradient-based optimization.
    \item \textbf{Nonlinear Equations:} Solving the system:
    \[
    (\vect{P} - \vect{L}) \cdot \vect{n}(\vect{P}) = 0, \quad \pi(\vect{P}) = \vect{p}
    \]
    is nonlinear and may have multiple or no valid solutions for $\lambda$.
    \item \textbf{Light Source Uncertainty:} The location of $\vect{L}$ is often unknown or only coarsely estimated. Inaccuracies here affect all derived constraints, particularly the tangency condition.
    \item \textbf{Shadow Detection in Images:} Extracting $\partial\Omega_{\text{shadow}}^{\text{obs}}$ from real images is sensitive to noise, soft shadow edges, and lighting variability, which may degrade the reliability of supervision signals like $\mathcal{L}_{\text{shadow}}$.
    \item \textbf{Complex Multi-Object Interactions:} Overlapping cast shadows, self-occlusion, and translucent effects complicate the interpretation of image shadows, making exact modeling infeasible without approximations.
\end{enumerate}

\begin{remark}[Need for Differentiable and Learnable Representations] \label{rmk:differentiable} ~\\
These challenges expose the limitations of pure geometric modeling. In practice, neural representations such as signed distance fields (SDFs) and differentiable ray tracing frameworks enable continuous and learnable formulations of shadow cues. This motivates models like ShadowNeuS, which incorporate shadow consistency into a trainable optimization loop, bridging geometry and learning.
\end{remark}









\newpage

\section*{References}
\begin{tabular}{@{}p{0.1\textwidth} p{0.9\textwidth}}
\hypertarget{[LWX23]}{[LWX23]} & Jingwang Ling, Zhibo Wang, Feng Xu. \textit{ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision}. arXiv: \href{https://arxiv.org/abs/2211.14086}{2211.14086}, 2023.
\end{tabular}

\end{document}